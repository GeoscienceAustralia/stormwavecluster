
# **Parsing of storm wave and tidal data, and conversion to a single time-series**
---------------------------------------------------------------------------------

*Gareth Davies, Geoscience Australia 2017*

# Introduction
------------------

This document illustrates the parsing of storm wave and tide data for our study
of storm wave statistics near the coastal town of Old Bar, NSW. It was
generated from the file [preprocess_data.Rmd](preprocess_data.Rmd), using the
literate programming R package *knitr*. 

If you have R installed, along with all the packages required to run this code,
and a copy of the *stormwavecluster* git repository, then you should be able to
re-run the analysis here by simply copy-pasting the code.

Alternatively, it can be run with the `knit` command in the *knitr* package: 
```{r useknitr, eval=FALSE}
library(knitr)
knit('preprocess_data.Rmd')
```

The basic approach followed here is to:
* **Step 1:** Parse relevant wave time-series data at a number of sites (all near-ish to the coastal town Old Bar, which was the site of interest for our study), and convert them to a single time-series representing waves at Old Bar. 
* **Step 2:** Parse tidal observations, and astronomical tidal predictions, for a site near Old Bar, and interpolate these onto the previous time-series.

In later parts of this analysis we will extract storm summary statistics from
the provided time-series, and statistically model their properties. 


# **Step 1: Parse the wave time-series data**
--------------------------------------------

# *Parse the measured wave time-series at Crowdy Head, Coffs Harbour and Sydney*
------------------------------------------------------------------------------------------

Here we parse measured wave time-series from Crowdy Head (since that is near
Old Bar, our primary site of interest). We also parse similar data measured at
Coffs Harbour and Sydney, for the purposes of filling gaps in the Crowdy Head
observations. 

**Below we source a script 'data_utilities.R'**, containing functions for various
parts of this analysis (for example: reading the wave data files; making plots
of the wave data; gap filling wave data). To see details of the latter steps, consult
the script. We put all the functions in 'data_utilities.R' into an enviroment
named `DU`. By using an environment, we avoid the possibility of accidentally
over-writing variables inside 'data_utilities.R'. 
```{r readDU}
DU = new.env() # Make a new environment
source('data_utilities.R', local=DU)  # Put the data_utilities.R functions in DU
```

Wave data for Crowdy Head, Coffs Harbour and Sydney was kindly provided by
Manly Hydraulics Laboratory for this study. It is included in this repository.
**Below we read wave data measured from 1985-2014.**
```{r get_1985_mhl_filenames}
mhl_wave_dir = '../../Data/NSW_Waves/'

# There are zipped csv files for each site, for the data from 1985-2014
mhl_wave_files = Sys.glob(paste0(mhl_wave_dir, '*1985*.csv.zip'))
mhl_wave_files

# Make a short name for each site
mhl_wave_sites = substr(basename(mhl_wave_files), 1, 4)
mhl_wave_sites

# Read the data
# wd = "wave data"
wd = DU$parse_MHL_wave_buoy_data(mhl_wave_sites, mhl_wave_files)

```

Next, we append data from 2015 to the above 1985-2014 data. We also read data
from another Sydney station which includes hindcast wave directions (based on
meteorological charts).

**Append the more recent wave data to `wd`**
```{r append_recent_mhl_data}
# Update: In early 2016 we received updated wave-buoy data
#
# Append the updated MHL wave buoy data
mhl_wave_files = Sys.glob(paste0(mhl_wave_dir, '*2016*.csv.zip'))
mhl_wave_sites = substr(basename(mhl_wave_files), 1, 4)
wd_update = DU$parse_MHL_wave_buoy_data(mhl_wave_sites, mhl_wave_files)

for(nm in names(wd)){
    matchInds = match(wd_update[[nm]]$time, wd[[nm]]$time)

    ## Lots of checks here

    # From graphical checks the overlapping data seems identical
    # So matchInds should be a sequence of consecutive integers, followed
    # by a sequence of NA's
    stopifnot( (max(which(!is.na(matchInds))) + 1) == min(which(is.na(matchInds))) )
    stopifnot( max(diff(matchInds), na.rm=TRUE)  == 1)
    stopifnot( min(diff(matchInds), na.rm=TRUE)  == 1)

    # Ensure names and overlapping data are identical
    stopifnot(all(names(wd_update[[nm]]) == names(wd[[nm]])))
    for(varname in names(wd_update[[nm]])){
        stopifnot(
            all(range(wd_update[[nm]][[varname]] - wd[[nm]][[varname]][matchInds], 
                    na.rm=TRUE) == 
                c(0, 0))
            )
    }

    # If all those tests have passed, we can update the data with an rbind 
    ll = matchInds[1] - 1
    wd[[nm]] = rbind(wd[[nm]][1:ll,], wd_update[[nm]])
}


# Get the other sydney data. It's in a different format, so we parse it here,
# and append it to the 'wd' list

syd1 = read.table(
    unz(description = paste0(mhl_wave_dir, 'SYDNOW 17-7-1987 to 4-10-2000.csv.zip'), 
        filename = 'SYDNOW 17-7-1987 to 4-10-2000.csv'),
    skip=7, header=TRUE, sep=",")

# Add a time variable
syd1$time = strptime(syd1$Date.Time, format='%d-%B-%Y %H:%M', tz='Etc/GMT-10')
# Add names to data.frame
names(syd1) = c('datetime', 'hsig', 'hmax', 'tz', 'tsig', 'tp1', 'dir', 'time')
# Add a year variable
syd1$year = DU$time_to_year(syd1$time)
# Append to 'wd' list under the name 'SYDL'
wd$SYDL = syd1
```

As a result of the above operations, we have a list `wd` containing data.frames
for each station. The station names and data dimensions are:
```{r stationnames}
# Station names
names(wd)
# Number of rows/columns for each station
lapply(wd, dim)
```
The data format looks like this (using the example of Crowdy Head):
```{r stationformat}
wd$CRHD[1:10,]
```
The data for the SYDL station actually has a few more columns. That does not
affect the analysis so long as it also contains columns with the same names and
data as observed at the other stations.
```{r stationformat_2}
wd$SYDL[1:10,]
```

The data_utilities.R script includes a function to plot the station data.
Here's an example. Note there are gaps in the data, which will be filled at
a later stage of the analysis.
```{r plot2013, fig.width=10, fig.height=7}
# Plot 2013 for Crowdy head
DU$wave_data_single_year_plot(year=2013, site='CRHD', wd=wd, max_hsig=8, max_tp1=15)
```

# *Convert the wave observations to a single 'Old Bar' time-series*
----------------------------------------------------------------------

Here we create a 'gap-filled' wave time-series which we will treat as
representative of wave conditions at Old Bar. The time-series consists of wave
properties measured in about 80m water depth, and tidal time-series measured
near the coast. 

Because Crowdy Head is much closer to Old Bar than the other wave measuring
sites, it is natural to take measurements at Crowdy Head as a 'first
preference' representation of waves at Old Bar. When Crowdy Head data was
missing, we decided to gap-fill preferentially with data from Coffs Harbour. If
the latter were missing, we used Sydney observations, firstly from the `SYDD` site
(the directional wave-rider buoy, measured since 1992), and secondly from the
`SYDL` site (the long-reef wave-rider measurements complemented with hind-cast
wave-directional data). 

These preferences were justified by comparison of observed wave height and
direction at the gap-filling stations with those from Crowdy Head during
high-wave events. A few plots comparing wave heights and directions are shown
below. 

**Compare wave directions during storms @ Crowdy Head with other stations, in 2013**
```{r direction_compare, fig.width=10, fig.height=5}
# Compare wave directions in a year, when waves at Crowdy Head exceed hsig_thresh
hsig_thresh = 2.9
year2compare = 2013

# We have put most of the comparison in a data_utilities.R function, for simplicity.
par(mfrow=c(1,2))
DU$check_station_correlations(year2compare, 'CRHD', 'SYDD', wd, 'dir', 
    site1_restriction = (wd$CRHD$hsig > hsig_thresh))
title(main='Wave direction at Crowdy Head vs Sydney', line=0.5)
DU$check_station_correlations(year2compare, 'CRHD', 'COFH', wd, 'dir', 
    site1_restriction = (wd$CRHD$hsig > hsig_thresh))
title(main='Wave direction at Crowdy Head vs Coffs Harbour', line=0.5)
# Note -- we do not have SYDL direction data from this year
```

**Compare significant wave height during storms @ Crowdy Head with other stations, in 2013**
```{r hsig_compare, fig.width=10, fig.height=10}
# Still using the same hsig_thresh and year2compare
par(mfrow=c(2,2))
DU$check_station_correlations(year2compare, 'CRHD', 'SYDD', wd, 'hsig',
    site1_restriction = (wd$CRHD$hsig > hsig_thresh))
title(main=bquote(H[sig] ~ 'at Crowdy Head vs Sydney'), line=0.5)
DU$check_station_correlations(year2compare, 'CRHD', 'COFH', wd, 'hsig',
    site1_restriction = (wd$CRHD$hsig > hsig_thresh))
title(main=bquote(H[sig] ~ 'at Crowdy Head vs Coffs Harbour'), line=0.5)
DU$check_station_correlations(1990, 'CRHD', 'SYDL', wd, 'hsig',
    site1_restriction = (wd$CRHD$hsig > hsig_thresh))
title(main=bquote(H[sig] ~ 'at Crowdy Head vs Long Reef'), line=0.5)
```


**Make the gap-filled wave data. It is stored in a variable named `full_data`**
```{r gap_filling}
# Get times to interpolate at
len_crhd = length(wd$CRHD$time)
desired_times = seq(wd$CRHD$time[1], wd$CRHD$time[len_crhd], by='hour')

# Get the interpolated 'full' data
site_preference_order = c('CRHD', 'COFH', 'SYDD', 'SYDL')
full_data = DU$gap_fill_wave_data(desired_times, site_preference_order, wd)
head(full_data)
tail(full_data)

# Append the 'full_data' to wd, and plot it
wd$full_data = full_data
```

**How much does each station contribute to the gap-filled wave data?**

The following plots show that most wave directions in `full_data` originate
from Crowdy Head, whereas most wave directions originate from the Sydney
waverider buoy. This is inevitable, because wave direction was only measured at
Crowdy Head and Coffs Harbour after ~ 2011, while measurements have been taken at
Sydney since 1992.
```{r gap_filling_check, fig.width=10, fig.height=5}
par(mfrow=c(1,2))
pie(table(full_data$waves_site), main='Source of wave data in Old Bar wave series')
pie(table(full_data$dir_site), main='Source of direction data in Old Bar wave series')
```


# **Step 2: Parse the tidal time-series data, and use astronomical tidal predictions to estimate the non-astronomical tidal residual**
--------------------------------------------------------------------------------------------------------------------------------------

The nearest tidal record to Old Bar is at Tomaree, Port Stephens. (Actually some
closer records do exist, but they were strongly affected by local seiching and
so judged unsuitable for representing regional conditions).

**Read the tomaree tidal data**
```{r parse_tides_tomaree, fig.width=10, fig.height=5}
tomaree_gauge_data = 
    '../../Data/NSW_Tides/TomareePW.csv.zip'
tidal_obs = DU$read_MHL_csv_tide_gauge(tomaree_gauge_data)
head(tidal_obs)
plot(tidal_obs$time, tidal_obs$tide, t='l', main='Tomaree tidal measurements')
```

The figure shows there are some gaps in the data.

**Get astronomical tidal predictions**. Next we get astronomical tidal
predictions for the same area, in order to estimate the astronomical tidal
residual. This makes use of our R interface to the TPXO7.2 tidal prediction
model, which turns out to work quite well along the NSW coast. 
```{r get_astro_tides}
# Use this variable to switch on/off use of TPXO72 interface. If it is not installed,
# we read the results I stored earlier. 
# Basically this is just a work-around for various installation problems that
# people might have (since tpxo72 is only tested on unix).
assume_tpxo72_is_installed = FALSE

if(assume_tpxo72_is_installed){
    # Get the R interface to TPXO72 -- note the 'chdir=TRUE' is important
    TPXO72 = new.env()
    source('../../R/tpxo7.2/BASIC_R_INTERFACE/predict_tide.R',
        chdir=TRUE, local=TPXO72)

    # Let's get the data near Tomaree for comparison -- this would be changed to
    # Old Bar for the final analysis -- but using Tomaree let's us compare with the
    # data, and check everything is ok

    site_name = 'tomaree' # This is just for convenience

    # Decimal degrees input.
    # Long,+152:10:56.06,,
    # Lat,-32:42:53.57,,
    site_coordinates = c(152 + 10/60 + 56.06/(60*60), -(32 + 42/60 + 53.57/(60*60)))

    # Set the output start-time and end-time (which can be in the past or future)
    #
    # NOTE: The start-time and end-time are in timezone 'Etc/GMT-10' according to
    # R's timezone database, which is the Sydney time-zone without daylight
    # savings (Beware: 'Etc/GMT-10' is what most people think of as 'GMT+10'). 
    #
    # This is because the tidal_obs$time vector is in that timezone (see the
    # function that created tidal_obs)
    #
    # It is important that the start_time / end_time specified have
    # the correct timezone -- since the tidal_prediction interface code converts
    # these times to GMT for TPXO72, and then reports back in the input timezone.
    #
    start_time = tidal_obs$time[1]
    end_time = tidal_obs$time[ length(tidal_obs$time) ]

    # For the time interval, I think choices like '2 hour' or '30 sec' or '5 days'
    # would be accepted as well.
    time_interval = '15 min' 

    # Use the R interface to get the tidal prediction
    tidal_pred = TPXO72$get_tidal_prediction(site_name, site_coordinates, 
        start_time, end_time, time_interval)

    #saveRDS(tidal_pred, '../../Data/NSW_Tides/tomaree_predictions.RDS')
}else{
    tidal_pred = readRDS('../../Data/NSW_Tides/tomaree_predictions.RDS')
}

head(tidal_pred)


```

**Compare astronomical predictions and measurements around June-September
2007**.  To check that the astronomical tidal predictions are reasonable, we
compare them with data during June-September 2007. We note a large storm
occurred in early June in this area (the Pasha-Bulker floods), which is
reflected in an increase in the computed tidal residual at this time (peaking
around 0.5 m, see figure below). In general, the tidal residual drifts around
zero in the figure, reflecting changes in atmospheric pressure and
oceanographic conditions, as well as smaller short-term errors in the tidal
prediction model. The residual is generally positive in the early part of the
figure (before mid July), and inspection of the data reveals this was a period
with numerous significant storm wave events. The agreement between the
astronomical model and observations is improved in the latter half of the
observational series, consistent with observations which suggest fewer storm
waves in this time.  However, it is worth noting that the tidal residual on
this coast is not purely related to storm wave activity, but also e.g. shelf
waves and seasonal factors, which have different origins.
```{r pasha-bulker, fig.width=10, fig.height=5}
# Plot June-September 2007 (Pasha-Bulker floods were in early June)
inds = which( (format(tidal_obs$time, '%Y')=='2007') &
              (format(tidal_obs$time, '%m')%in%c('06', '07', '08', '09')) )
mean_tidal_obs = mean(tidal_obs$tide, na.rm=TRUE)

# Compute the tidal residual. For these datasets the predicted and observed
# times should be identical -- so here we test that this is true, then subtract
# the 2 levels. Interpolation is required in the general case
stopifnot(all(tidal_obs$time == tidal_pred$time))
tidal_residual = (tidal_obs$tide - tidal_pred$tide) - mean_tidal_obs

# Summarise the tidal residual distribution
summary(tidal_residual)

plot(tidal_obs$time[inds], tidal_obs$tide[inds] - mean_tidal_obs, t='l',
    xlab='Time', ylab='Stage (m MSL)', main='Tomaree tides, June-September 2007', 
    cex.main=2)
points(tidal_pred$time, tidal_pred$tide, t='l', col='red')
points(tidal_obs$time[inds], tidal_residual[inds], t='l', col='green')
grid(col='brown')
legend('topright', c('Measured', 'Astronomical Predictions', 'Residual'), 
    lty=c(1,1,1), col = c('black', 'red', 'green'), bg='white')
```



**Interpolate the measured tides and astronomical tidal residual onto the wave data series**.
We interpolate the observed tide and the tidal residual onto the hourly
storm wave data (`full_data`). Because their are gaps in the tidal data, we
only interpolate if the time difference between the nearest tidal observation
and the interpolation time is less than 1.5 hours. Otherwise, the tide and
surge are denoted as `NA` (missing data).
```{r put_tides_in_full_data, fig.width=10, fig.height=5}
# Make a function to interpolate the tidal observations
# Note this will 'interpolate over' missing data
tidal_data_fun = approxfun(tidal_obs$time, tidal_obs$tide - mean_tidal_obs)

# Find where we have missing tidal data. 
t0 = as.numeric(julian(full_data$time))
t1 = as.numeric(julian(tidal_obs$time))
source('../../R/nearest_index_sorted/nearest_index_sorted_cpp.R', local=TRUE)
nearest_tidalobs_ind = nearest_index_sorted_cpp(t1, t0, check_is_sorted = 1)

# Say "if the time-difference between the nearest tidal observation and the
# full_data time is > 1.5 hours", then we are missing data
full_data_missing_tidal_obs = which( 
    (abs(t0 - t1[nearest_tidalobs_ind]) > (1.5/24)) |
    (is.na(tidal_obs$tide[nearest_tidalobs_ind]))
    )

# Append interpolated values to the full_data (which has 1hour spacing)
full_data$tide = tidal_data_fun(full_data$time)
full_data$tide[full_data_missing_tidal_obs] = NA
# Do the same for the tidal_residual
tidal_resid_fun = approxfun(tidal_obs$time, tidal_residual)
full_data$tideResid = tidal_resid_fun(full_data$time)
full_data$tideResid[full_data_missing_tidal_obs] = NA
head(full_data)

# Overwrite the old full_data in 'wd'
wd$full_data = full_data

# Check it -- overplot the interpolated and original tidal data, to see that they agree
plot(tidal_obs$time[inds[1:500]], tidal_obs$tide[inds[1:500]] - mean_tidal_obs, t='l',
    xlab='Time', ylab='Stage (m MSL)', main='Graphical check that the interpolation worked')
points(full_data$time, full_data$tide, t='l', col='blue')
legend('topright', c('Observations (15min)', 'Interpolated observations (1hr)'), 
    col=c('black', 'blue'), lty=c(1,1), bg='white')

# Clean up temporary variables
rm(inds, tidal_resid_fun)
```

**Save the current session for later usage**. We actually save the session with
all variables, and then remove all non-essential variables and save again.
```{r saveImageETC}
# Save the current state of R
save.image('Rimages/Session_data_processing.Rdata')
# It may be easier to work with a simplified version of the session.

# List all variables in the workspace
ls()

# Variables to keep (note library packages will be kept anyway)
keepVars = c('full_data', 'DU', 'wd')

# Remove everything except the variables named in keepVars
rm(list = setdiff(ls(), keepVars))

# Check what we have now
ls()

# Save an image with just the remaining variables
save.image('Rimages/Session_data_processing_clean.Rdata')
```

## **Moving on**

The next step of this vignette is in [extract_storm_events.md](extract_storm_events.md).
